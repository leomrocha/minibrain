{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Leo's Home page](https://leomrocha.github.com) -- [Github Page](https://github.com/leomrocha/minibrain/blob/master/sensors/image) -- License: [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with Image Multi Resolution Convolutional Autoencoders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Leonardo M. Rocha](https://leomrocha.github.com)\n",
    "\n",
    "[Contact Me](https://leomrocha.github.io/contact/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents a study I did in 2018 on Multi Resolution Convolutional Autoencoders where the input image is scaled down and passed as input, and the latent space is composed of the combined latent space of the different Convolutional Encoders.\n",
    "\n",
    "This experience was meant to be the first step into exploring foveal-like perception but I never created the agent to deal with it (too complex for me at that moment with the knowledge, resources and time available).\n",
    "\n",
    "The experiment was successful in the sense that I learned about image autoencoders and managed to create different versions.\n",
    "\n",
    "I leave this code available, the only modifications are just some code adaptations to make it work with pytorch v1.7 as there were a couple of deprecated things.\n",
    "\n",
    "All the code is available at [minibrain](https://github.com/leomrocha/minibrain/blob/master/sensors/image/multi_res_cae.py)\n",
    "\n",
    "Feel free to play with it if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography:\n",
    "\n",
    "* [Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction](http://people.idsia.ch/~ciresan/data/icann2011.pdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# import skimage \n",
    "import math\n",
    "# import io\n",
    "# import requests\n",
    "# from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from helper_modules import *\n",
    "from multi_res_cae import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some tricks in jupyter to do autoreload of the modules when they are modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport helpers, helper_modules, multi_res_cae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "helper_modules helpers multi_res_cae\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%aimport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current notebook only contains the parameters I've chosen at the end, but there were many tests done, some examples are left as comments because they give more information on what was tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "# num_epochs = 5\n",
    "# batch_size = 100\n",
    "# learning_rate = 0.001\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.64 s, sys: 516 ms, total: 2.15 s\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#%time model = MultiFullCAE(in_img_shape=(32,32), full_image_resize=(24,24)).cuda()\n",
    "model = MultiResCAE(in_img_shape=[32,32], channels=3, conv_layer_feat=[16, 32, 64],\n",
    "                 res_px=[[24, 24], [16, 16], [12, 12]], crop_sizes=[[32, 32], [24,24], [12, 12]],\n",
    "                 # conv_sizes = [(3,5,7), (3,5,7,11), (3,5,7,11)]  # this is too much I think\n",
    "                 # conv_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5, 7]]  # test b\n",
    "#                  conv_sizes=[[5, 7, 11], [3, 5, 7, 9], [1, 3, 5]]  # test c\n",
    "                 conv_sizes=[[5, 7], [3, 5, 7], [1, 3, 5]]  # test d\n",
    "        ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.28 ms, sys: 0 ns, total: 1.28 ms\n",
      "Wall time: 1.29 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 3, 32, 32)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CPU times: user 736 ms, sys: 179 ms, total: 914 ms\n",
      "Wall time: 921 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#transformation = monochrome_preprocess(32,32)\n",
    "transformation = fullimage_preprocess(32,32)\n",
    "#train_loader, test_loader = get_loaders(batch_size, transformation, dataset=datasets.CocoDetection)\n",
    "train_loader, test_loader = get_loaders(batch_size, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/20], loss:0.5369\n",
      "epoch [2/20], loss:0.5012\n",
      "epoch [3/20], loss:0.3744\n",
      "epoch [4/20], loss:0.3535\n",
      "epoch [5/20], loss:0.4147\n",
      "epoch [6/20], loss:0.4017\n",
      "epoch [7/20], loss:0.3398\n",
      "epoch [8/20], loss:0.3981\n",
      "epoch [9/20], loss:0.3920\n",
      "epoch [10/20], loss:0.3525\n",
      "epoch [11/20], loss:0.3438\n",
      "epoch [12/20], loss:0.3502\n",
      "epoch [13/20], loss:0.3335\n",
      "epoch [14/20], loss:0.3064\n",
      "epoch [15/20], loss:0.3982\n",
      "epoch [16/20], loss:0.3694\n",
      "epoch [17/20], loss:0.3668\n",
      "epoch [18/20], loss:0.3373\n",
      "epoch [19/20], loss:0.3481\n",
      "epoch [20/20], loss:0.3426\n",
      "CPU times: user 12min 44s, sys: 3min 7s, total: 15min 51s\n",
      "Wall time: 15min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (img, labels) in enumerate(train_loader):\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "#         print(\"encoding batch of  images\")\n",
    "        output = model(img)\n",
    "#         print(\"computing loss\")\n",
    "        loss = criterion(output, img)\n",
    "        # ===================backward====================\n",
    "#         print(\"Backward \")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.data))\n",
    "    if epoch % 4 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        in_pic = to_img(img.cpu().data)\n",
    "        save_image(pic, './mrcae_results/e_in-32x32_1-3-5_7-out_image_{}.png'.format(epoch))\n",
    "        save_image(in_pic, './mrcae_results/e_in-32x32_1-3-5_7-in_image_{}.png'.format(epoch))\n",
    "#     if loss.data[0] < 0.35: #arbitrary number because I saw that it works well enough\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_in-32x32_1-3-5_7-in_image_0.png   e_in-32x32_1-3-5_7-out_image_0.png\n",
      "e_in-32x32_1-3-5_7-in_image_12.png  e_in-32x32_1-3-5_7-out_image_12.png\n",
      "e_in-32x32_1-3-5_7-in_image_16.png  e_in-32x32_1-3-5_7-out_image_16.png\n",
      "e_in-32x32_1-3-5_7-in_image_4.png   e_in-32x32_1-3-5_7-out_image_4.png\n",
      "e_in-32x32_1-3-5_7-in_image_8.png   e_in-32x32_1-3-5_7-out_image_8.png\n"
     ]
    }
   ],
   "source": [
    "!ls mrcae_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(\"fmrcae_in-64x64_32x32_3-5-7-11.pth\", model)\n",
    "#torch.save(\"mrcae_in-32x32_.pth\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and output of the first epoch\n",
    "\n",
    "![input](mrcae_results/e_in-32x32_1-3-5_7-in_image_0.png)\n",
    "![output](mrcae_results/e_in-32x32_1-3-5_7-out_image_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and output of the last saved epoch\n",
    "\n",
    "![input](mrcae_results/e_in-32x32_1-3-5_7-in_image_16.png)\n",
    "![output](mrcae_results/e_in-32x32_1-3-5_7-out_image_16.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
