{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# import skimage \n",
    "import math\n",
    "# import io\n",
    "# import requests\n",
    "# from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "\n",
    "* [Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction](http://people.idsia.ch/~ciresan/data/icann2011.pdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostly Taken from examples here:\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "# https://github.com/csgwon/pytorch-deconvnet/blob/master/models/vgg16_deconv.py\n",
    "# Other resources\n",
    "# https://github.com/pgtgrly/Convolution-Deconvolution-Network-Pytorch/blob/master/conv_deconv.py\n",
    "# https://github.com/kvfrans/variational-autoencoder\n",
    "# https://github.com/SherlockLiao/pytorch-beginner/blob/master/08-AutoEncoder/conv_autoencoder.py\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main-gpu.py\n",
    "# https://pgaleone.eu/neural-networks/2016/11/24/convolutional-autoencoders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAEEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Encoder = Q(z|X) for the Network\n",
    "    \"\"\"\n",
    "    def __init__(self, w,h, channels=3, hid_dim=500, code_dim=200, kernel_size=3, first_feature_count=16):\n",
    "        super(CAEEncoder, self).__init__()\n",
    "        self.indices = []\n",
    "        padding = math.floor(kernel_size/2)\n",
    "        l1_feat = first_feature_count\n",
    "        l2_feat = l1_feat * 2\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(channels, l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.BatchNorm2d(l1_feat),\n",
    "            nn.ReLU(),\n",
    "#             nn.Conv2d(l1_feat, l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "# #             nn.BatchNorm2d(l1_feat),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(l1_feat, l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "# #             nn.BatchNorm2d(l1_feat),\n",
    "#             nn.ReLU(),\n",
    "            nn.Conv2d(l1_feat, l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.BatchNorm2d(l1_feat),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(l1_feat, l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.BatchNorm2d(l2_feat),\n",
    "            nn.ReLU(),\n",
    "#             nn.Conv2d(l2_feat, l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "# #             nn.BatchNorm2d(l2_feat),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(l2_feat, l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "# #             nn.BatchNorm2d(l2_feat),\n",
    "#             nn.ReLU(),\n",
    "            nn.Conv2d(l2_feat, l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.BatchNorm2d(l2_feat),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        )\n",
    "        self.conv_dim = int(((w*h)/16) * l2_feat)\n",
    "        #self.conv_dim = int( channels * (w/4) * l2_feat)\n",
    "        self.fc1 = nn.Linear(self.conv_dim, hid_dim)\n",
    "        self.fc2 = nn.Linear(hid_dim, code_dim)\n",
    "#         self.fc1 = nn.Linear(576, hid_dim)\n",
    "\n",
    "    def get_conv_layer_indices(self):\n",
    "        return [0, 2, 5, 7, 10]  # without BatchNorm2d\n",
    "        #return [0, 3, 7, 10, 14]  # with BatchNorm2d\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.indices = []\n",
    "#         print(\"encoding conv l1\")\n",
    "        out, idx  = self.layer1(x)\n",
    "        self.indices.append(idx)\n",
    "#         print(\"encoding conv l2\")\n",
    "        out, idx = self.layer2(out)\n",
    "        self.indices.append(idx)\n",
    "#         print(out.size(), self.conv_dim)\n",
    "#         print(\"view for  FC l1\")\n",
    "        out = out.view(out.size(0), -1)\n",
    "#         print(out.size())\n",
    "#         print(\"encoding FC1 \")\n",
    "        out = self.fc1(out)\n",
    "#         print(\"encoding FC2 \")\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAEDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The Decoder = P(X|z) for the Network\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, width, height, channels=3, hid_dim=500, code_dim=200, kernel_size=3, first_feature_count=16):\n",
    "        super(CAEDecoder, self).__init__()\n",
    "        padding = math.floor(kernel_size/2)\n",
    "#         self. width = width\n",
    "#         self.height = height\n",
    "#         self.channels = channels\n",
    "        self.encoder = encoder\n",
    "        self.w_conv_dim = int(width/4)\n",
    "        self.h_conv_dim = int(height/4)\n",
    "        self.l1_feat = first_feature_count\n",
    "        self.l2_feat = self.l1_feat * 2\n",
    "        self.conv_dim = int(((width*height)/16) * self.l2_feat)\n",
    "        #self.conv_dim = int(channels * (width/4) * self.l2_feat)\n",
    "        self.layer1 = torch.nn.Linear(code_dim, hid_dim)\n",
    "        self.layer2 = torch.nn.Linear(hid_dim, self.conv_dim)\n",
    "        self.unpool_1 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv_layer_1 = torch.nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.l2_feat, self.l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(self.l2_feat, self.l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(self.l2_feat, self.l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.ReLU(),\n",
    "            nn.ConvTranspose2d(self.l2_feat, self.l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.unpool_2 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv_layer_2 = torch.nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.l1_feat, self.l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(self.l1_feat, self.l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(self.l1_feat, self.l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.ReLU(),\n",
    "            nn.ConvTranspose2d(self.l1_feat, channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "#         print(\"decoding l1\")\n",
    "        out = F.relu(self.layer1(x))\n",
    "#         print(\"decoding l2\")\n",
    "        out = F.relu(self.layer2(out))\n",
    "#         print(out.size(), self.conv_dim)\n",
    "#         print(\"changing tensor shape to be an image\")\n",
    "        out = out.view(out.size(0), self.l2_feat, self.w_conv_dim, self.h_conv_dim)\n",
    "        out = self.unpool_1(out, self.encoder.indices[-1])\n",
    "#         print(out.size())\n",
    "#         print(\"decoding c1\")\n",
    "        out = self.deconv_layer_1(out)\n",
    "#         print(\"decoding c2\")\n",
    "        out = self.unpool_2(out, self.encoder.indices[-2])\n",
    "        out = self.deconv_layer_2(out)\n",
    "#         print(\"returning decoder response\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    def __init__(self, width, height, channels, hid_dim=500, code_dim=200, conv_layer_feat=16):\n",
    "        super(CAE, self).__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channels = channels\n",
    "        self.encoder = CAEEncoder(width, height, channels, hid_dim, code_dim, 3, conv_layer_feat)\n",
    "        self.decoder = CAEDecoder(self.encoder, width, height, channels, hid_dim, code_dim, 3, conv_layer_feat)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "        return out\n",
    "        \n",
    "    def save_model(self, name, path):\n",
    "        torch.save(self.encoder, os.path.join(path, \"cae_encoder_\"+name+\".pth\"))\n",
    "        torch.save(self.decoder, os.path.join(path, \"cae_decoder_\"+name+\".pth\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definitions of the operations for the full image autoencoder\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406], # from example here https://github.com/pytorch/examples/blob/409a7262dcfa7906a92aeac25ee7d413baa88b67/imagenet/main.py#L94-L95\n",
    "   std=[0.229, 0.224, 0.225]\n",
    "#   mean=[0.5, 0.5, 0.5], # from example here http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "#    std=[0.5, 0.5, 0.5]\n",
    ")\n",
    "\n",
    "#the whole image gets resized to a small image that can be quickly analyzed to get important points\n",
    "def fullimage_preprocess(w=48,h=48):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((w,h)), #this should be used ONLY if the image is bigger than this size\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "#the full resolution fovea just is a small 12x12 patch \n",
    "full_resolution_crop = transforms.Compose([\n",
    "    transforms.RandomCrop(12),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "    ])\n",
    "\n",
    "def downsampleTensor(crop_size, final_size=16):\n",
    "    sample = transforms.Compose([\n",
    "        transforms.RandomCrop(crop_size),\n",
    "        transforms.Resize(final_size), \n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size, transformation, dataset = datasets.CIFAR100, cuda=True):\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset('../data', train=True, download=True,\n",
    "                       transform=transformation),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset('../data', train=False, transform=transformation),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "# num_epochs = 5\n",
    "# batch_size = 100\n",
    "# learning_rate = 0.001\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CAE(12,12,3,500,200,32).cuda()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of CAE(\n",
       "  (encoder): CAEEncoder(\n",
       "    (layer1): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (fc1): Linear(in_features=576, out_features=500, bias=True)\n",
       "    (fc2): Linear(in_features=500, out_features=200, bias=True)\n",
       "  )\n",
       "  (decoder): CAEDecoder(\n",
       "    (encoder): CAEEncoder(\n",
       "      (layer1): Sequential(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (fc1): Linear(in_features=576, out_features=500, bias=True)\n",
       "      (fc2): Linear(in_features=500, out_features=200, bias=True)\n",
       "    )\n",
       "    (layer1): Linear(in_features=200, out_features=500, bias=True)\n",
       "    (layer2): Linear(in_features=500, out_features=576, bias=True)\n",
       "    (unpool_1): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "    (deconv_layer_1): Sequential(\n",
       "      (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (unpool_2): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "    (deconv_layer_2): Sequential(\n",
       "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 3, 12, 12)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformation = full_resolution_crop\n",
    "train_loader, test_loader = get_loaders(batch_size, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.2865\n",
      "epoch [2/100], loss:0.2521\n",
      "epoch [3/100], loss:0.3281\n",
      "epoch [4/100], loss:0.2390\n",
      "epoch [5/100], loss:0.2846\n",
      "epoch [6/100], loss:0.2508\n",
      "epoch [7/100], loss:0.2427\n",
      "epoch [8/100], loss:0.2595\n",
      "epoch [9/100], loss:0.2347\n",
      "epoch [10/100], loss:0.2019\n",
      "epoch [11/100], loss:0.2335\n",
      "epoch [12/100], loss:0.1997\n",
      "epoch [13/100], loss:0.2062\n",
      "epoch [14/100], loss:0.2735\n",
      "epoch [15/100], loss:0.2377\n",
      "epoch [16/100], loss:0.2347\n",
      "epoch [17/100], loss:0.2409\n",
      "epoch [18/100], loss:0.2986\n",
      "epoch [19/100], loss:0.1872\n",
      "epoch [20/100], loss:0.2455\n",
      "epoch [21/100], loss:0.2037\n",
      "epoch [22/100], loss:0.1987\n",
      "epoch [23/100], loss:0.2376\n",
      "epoch [24/100], loss:0.2354\n",
      "epoch [25/100], loss:0.2565\n",
      "epoch [26/100], loss:0.3044\n",
      "epoch [27/100], loss:0.2015\n",
      "epoch [28/100], loss:0.1975\n",
      "epoch [29/100], loss:0.2486\n",
      "epoch [30/100], loss:0.2302\n",
      "epoch [31/100], loss:0.1814\n",
      "epoch [32/100], loss:0.2905\n",
      "epoch [33/100], loss:0.2466\n",
      "epoch [34/100], loss:0.2186\n",
      "epoch [35/100], loss:0.2765\n",
      "epoch [36/100], loss:0.2326\n",
      "epoch [37/100], loss:0.2629\n",
      "epoch [38/100], loss:0.1888\n",
      "epoch [39/100], loss:0.1808\n",
      "epoch [40/100], loss:0.2828\n",
      "epoch [41/100], loss:0.2099\n",
      "epoch [42/100], loss:0.1531\n",
      "epoch [43/100], loss:0.2162\n",
      "epoch [44/100], loss:0.3074\n",
      "epoch [45/100], loss:0.2446\n",
      "epoch [46/100], loss:0.2313\n",
      "epoch [47/100], loss:0.1946\n",
      "epoch [48/100], loss:0.2143\n",
      "epoch [49/100], loss:0.2160\n",
      "epoch [50/100], loss:0.2128\n",
      "epoch [51/100], loss:0.2326\n",
      "epoch [52/100], loss:0.2121\n",
      "epoch [53/100], loss:0.1943\n",
      "epoch [54/100], loss:0.2527\n",
      "epoch [55/100], loss:0.2467\n",
      "epoch [56/100], loss:0.1780\n",
      "epoch [57/100], loss:0.1740\n",
      "epoch [58/100], loss:0.2465\n",
      "epoch [59/100], loss:0.2868\n",
      "epoch [60/100], loss:0.1915\n",
      "epoch [61/100], loss:0.2127\n",
      "epoch [62/100], loss:0.2066\n",
      "epoch [63/100], loss:0.2195\n",
      "epoch [64/100], loss:0.1878\n",
      "epoch [65/100], loss:0.2023\n",
      "epoch [66/100], loss:0.2384\n",
      "epoch [67/100], loss:0.2312\n",
      "epoch [68/100], loss:0.1760\n",
      "epoch [69/100], loss:0.2046\n",
      "epoch [70/100], loss:0.1693\n",
      "epoch [71/100], loss:0.2427\n",
      "epoch [72/100], loss:0.1739\n",
      "epoch [73/100], loss:0.1770\n",
      "epoch [74/100], loss:0.2504\n",
      "epoch [75/100], loss:0.1955\n",
      "epoch [76/100], loss:0.2028\n",
      "epoch [77/100], loss:0.1623\n",
      "epoch [78/100], loss:0.2135\n",
      "epoch [79/100], loss:0.2217\n",
      "epoch [80/100], loss:0.2665\n",
      "epoch [81/100], loss:0.1907\n",
      "epoch [82/100], loss:0.2280\n",
      "epoch [83/100], loss:0.2509\n",
      "epoch [84/100], loss:0.2566\n",
      "epoch [85/100], loss:0.2563\n",
      "epoch [86/100], loss:0.1809\n",
      "epoch [87/100], loss:0.2217\n",
      "epoch [88/100], loss:0.1991\n",
      "epoch [89/100], loss:0.2814\n",
      "epoch [90/100], loss:0.2397\n",
      "epoch [91/100], loss:0.2347\n",
      "epoch [92/100], loss:0.2026\n",
      "epoch [93/100], loss:0.2265\n",
      "epoch [94/100], loss:0.1836\n",
      "epoch [95/100], loss:0.2653\n",
      "epoch [96/100], loss:0.2468\n",
      "epoch [97/100], loss:0.1904\n",
      "epoch [98/100], loss:0.2006\n",
      "epoch [99/100], loss:0.1919\n",
      "epoch [100/100], loss:0.2274\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CAE/cae_encoder_2x2-2xfc-layer.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-11d9d317b389>\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(self, name, path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cae_encoder_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cae_decoder_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CAE/cae_encoder_2x2-2xfc-layer.pth'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (img, labels) in enumerate(train_loader):\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "#         print(\"encoding batch of  images\")\n",
    "        output = model(img)\n",
    "#         print(\"computing loss\")\n",
    "        loss = criterion(output, img)\n",
    "        # ===================backward====================\n",
    "#         print(\"Backward \")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.data))\n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        in_pic = to_img(img.cpu().data)\n",
    "        save_image(pic, './cae_results/2x2-2xfc-out_image_{}.png'.format(epoch))\n",
    "        save_image(in_pic, './cae_results/2x2-2xfc-in_image_{}.png'.format(epoch))\n",
    "    if loss.data < 0.15: #arbitrary number because I saw that it works well enough\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"2x2-2xfc-layer\", \"CAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Results \n",
    "\n",
    "Experiments with the following configurations:\n",
    "- 2 layers with 2 convolutional stages each  <- **best result**\n",
    "- 2 layers with 2 convolutional stages each and 2 fully connected layers  <- bigger model and a bit slower to converge, but results are good too\n",
    "- 2 layers with 2 convolutional stages each with batch normalization\n",
    "- 2 layers with 4 convolutional stages each <- **worst result**\n",
    "\n",
    "\n",
    "2 layers with 4 conv stages each does not give the same results as 2 layers with 2 conv stages\n",
    "\n",
    "It not only converges MUCH faster and the models are smaller, but the actually the convergence is much better\n",
    "\n",
    "For batch normalization happens the same, without batchnorm2d converges faster and model is smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
