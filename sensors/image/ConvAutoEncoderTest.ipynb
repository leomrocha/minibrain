{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Leo's Home page](https://leomrocha.github.com) -- [Github Page](https://github.com/leomrocha/minibrain/blob/master/sensors/image) -- License: [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with Image Convolutional Autoencoders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Leonardo M. Rocha](https://leomrocha.github.com)\n",
    "\n",
    "[Contact Me](https://leomrocha.github.io/contact/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook presents some experimentation I did in 2018 with Convolutional Autoencoders.\n",
    "\n",
    "All the source code of the experiments (working _and_ broken) is available in the [Github project](https://github.com/leomrocha/minibrain/blob/master/sensors/image)\n",
    "\n",
    "There is no much more text in this notebook except for some words at the end, as the source code and comments should be enough to explain how and why things work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# import skimage \n",
    "import math\n",
    "# import io\n",
    "# import requests\n",
    "# from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography:\n",
    "\n",
    "* [Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction](http://people.idsia.ch/~ciresan/data/icann2011.pdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples:\n",
    "\n",
    "* https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "* https://github.com/csgwon/pytorch-deconvnet/blob/master/models/vgg16_deconv.py\n",
    "\n",
    "\n",
    "### Other resources\n",
    "\n",
    "* https://github.com/pgtgrly/Convolution-Deconvolution-Network-Pytorch/blob/master/conv_deconv.py\n",
    "* https://github.com/kvfrans/variational-autoencoder\n",
    "* https://github.com/SherlockLiao/pytorch-beginner/blob/master/08-AutoEncoder/conv_autoencoder.py\n",
    "* https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main-gpu.py\n",
    "* https://pgaleone.eu/neural-networks/2016/11/24/convolutional-autoencoders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAEEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Encoder = Q(z|X) for the Network\n",
    "    \"\"\"\n",
    "    def __init__(self, w,h, channels=3, hid_dim=500, code_dim=200, kernel_size=3, first_feature_count=16):\n",
    "        super(CAEEncoder, self).__init__()\n",
    "        self.indices = []\n",
    "        padding = math.floor(kernel_size/2)\n",
    "        l1_feat = first_feature_count\n",
    "        l2_feat = l1_feat * 2\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(channels, l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.BatchNorm2d(l1_feat),\n",
    "            nn.ReLU(),\n",
    "#             nn.Conv2d(l1_feat, l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "# #             nn.BatchNorm2d(l1_feat),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(l1_feat, l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "# #             nn.BatchNorm2d(l1_feat),\n",
    "#             nn.ReLU(),\n",
    "            nn.Conv2d(l1_feat, l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.BatchNorm2d(l1_feat),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(l1_feat, l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.BatchNorm2d(l2_feat),\n",
    "            nn.ReLU(),\n",
    "#             nn.Conv2d(l2_feat, l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "# #             nn.BatchNorm2d(l2_feat),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(l2_feat, l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "# #             nn.BatchNorm2d(l2_feat),\n",
    "#             nn.ReLU(),\n",
    "            nn.Conv2d(l2_feat, l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.BatchNorm2d(l2_feat),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        )\n",
    "        self.conv_dim = int(((w*h)/16) * l2_feat)\n",
    "        #self.conv_dim = int( channels * (w/4) * l2_feat)\n",
    "        self.fc1 = nn.Linear(self.conv_dim, hid_dim)\n",
    "        self.fc2 = nn.Linear(hid_dim, code_dim)\n",
    "#         self.fc1 = nn.Linear(576, hid_dim)\n",
    "\n",
    "    def get_conv_layer_indices(self):\n",
    "        return [0, 2, 5, 7, 10]  # without BatchNorm2d\n",
    "        #return [0, 3, 7, 10, 14]  # with BatchNorm2d\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.indices = []\n",
    "#         print(\"encoding conv l1\")\n",
    "        out, idx  = self.layer1(x)\n",
    "        self.indices.append(idx)\n",
    "#         print(\"encoding conv l2\")\n",
    "        out, idx = self.layer2(out)\n",
    "        self.indices.append(idx)\n",
    "#         print(out.size(), self.conv_dim)\n",
    "#         print(\"view for  FC l1\")\n",
    "        out = out.view(out.size(0), -1)\n",
    "#         print(out.size())\n",
    "#         print(\"encoding FC1 \")\n",
    "        out = self.fc1(out)\n",
    "#         print(\"encoding FC2 \")\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAEDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The Decoder = P(X|z) for the Network\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, width, height, channels=3, hid_dim=500, code_dim=200, kernel_size=3, first_feature_count=16):\n",
    "        super(CAEDecoder, self).__init__()\n",
    "        padding = math.floor(kernel_size/2)\n",
    "#         self. width = width\n",
    "#         self.height = height\n",
    "#         self.channels = channels\n",
    "        self.encoder = encoder\n",
    "        self.w_conv_dim = int(width/4)\n",
    "        self.h_conv_dim = int(height/4)\n",
    "        self.l1_feat = first_feature_count\n",
    "        self.l2_feat = self.l1_feat * 2\n",
    "        self.conv_dim = int(((width*height)/16) * self.l2_feat)\n",
    "        #self.conv_dim = int(channels * (width/4) * self.l2_feat)\n",
    "        self.layer1 = torch.nn.Linear(code_dim, hid_dim)\n",
    "        self.layer2 = torch.nn.Linear(hid_dim, self.conv_dim)\n",
    "        self.unpool_1 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv_layer_1 = torch.nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.l2_feat, self.l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(self.l2_feat, self.l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(self.l2_feat, self.l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.ReLU(),\n",
    "            nn.ConvTranspose2d(self.l2_feat, self.l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.unpool_2 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv_layer_2 = torch.nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.l1_feat, self.l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(self.l1_feat, self.l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(self.l1_feat, self.l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.ReLU(),\n",
    "            nn.ConvTranspose2d(self.l1_feat, channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "#         print(\"decoding l1\")\n",
    "        out = F.relu(self.layer1(x))\n",
    "#         print(\"decoding l2\")\n",
    "        out = F.relu(self.layer2(out))\n",
    "#         print(out.size(), self.conv_dim)\n",
    "#         print(\"changing tensor shape to be an image\")\n",
    "        out = out.view(out.size(0), self.l2_feat, self.w_conv_dim, self.h_conv_dim)\n",
    "        out = self.unpool_1(out, self.encoder.indices[-1])\n",
    "#         print(out.size())\n",
    "#         print(\"decoding c1\")\n",
    "        out = self.deconv_layer_1(out)\n",
    "#         print(\"decoding c2\")\n",
    "        out = self.unpool_2(out, self.encoder.indices[-2])\n",
    "        out = self.deconv_layer_2(out)\n",
    "#         print(\"returning decoder response\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    def __init__(self, width, height, channels, hid_dim=500, code_dim=200, conv_layer_feat=16):\n",
    "        super(CAE, self).__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channels = channels\n",
    "        self.encoder = CAEEncoder(width, height, channels, hid_dim, code_dim, 3, conv_layer_feat)\n",
    "        self.decoder = CAEDecoder(self.encoder, width, height, channels, hid_dim, code_dim, 3, conv_layer_feat)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "        return out\n",
    "        \n",
    "    def save_model(self, name, path):\n",
    "        torch.save(self.encoder, os.path.join(path, \"cae_encoder_\"+name+\".pth\"))\n",
    "        torch.save(self.decoder, os.path.join(path, \"cae_decoder_\"+name+\".pth\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definitions of the operations for the full image autoencoder\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406], # from example here https://github.com/pytorch/examples/blob/409a7262dcfa7906a92aeac25ee7d413baa88b67/imagenet/main.py#L94-L95\n",
    "   std=[0.229, 0.224, 0.225]\n",
    "#   mean=[0.5, 0.5, 0.5], # from example here http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "#    std=[0.5, 0.5, 0.5]\n",
    ")\n",
    "\n",
    "#the whole image gets resized to a small image that can be quickly analyzed to get important points\n",
    "def fullimage_preprocess(w=48,h=48):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((w,h)), #this should be used ONLY if the image is bigger than this size\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "#the full resolution fovea just is a small 12x12 patch \n",
    "full_resolution_crop = transforms.Compose([\n",
    "    transforms.RandomCrop(12),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "    ])\n",
    "\n",
    "def downsampleTensor(crop_size, final_size=16):\n",
    "    sample = transforms.Compose([\n",
    "        transforms.RandomCrop(crop_size),\n",
    "        transforms.Resize(final_size), \n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size, transformation, dataset = datasets.CIFAR100, cuda=True):\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset('../data', train=True, download=True,\n",
    "                       transform=transformation),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset('../data', train=False, transform=transformation),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "# num_epochs = 5\n",
    "# batch_size = 100\n",
    "# learning_rate = 0.001\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CAE(12,12,3,500,200,32).cuda()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of CAE(\n",
       "  (encoder): CAEEncoder(\n",
       "    (layer1): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (fc1): Linear(in_features=576, out_features=500, bias=True)\n",
       "    (fc2): Linear(in_features=500, out_features=200, bias=True)\n",
       "  )\n",
       "  (decoder): CAEDecoder(\n",
       "    (encoder): CAEEncoder(\n",
       "      (layer1): Sequential(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (fc1): Linear(in_features=576, out_features=500, bias=True)\n",
       "      (fc2): Linear(in_features=500, out_features=200, bias=True)\n",
       "    )\n",
       "    (layer1): Linear(in_features=200, out_features=500, bias=True)\n",
       "    (layer2): Linear(in_features=500, out_features=576, bias=True)\n",
       "    (unpool_1): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "    (deconv_layer_1): Sequential(\n",
       "      (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (unpool_2): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "    (deconv_layer_2): Sequential(\n",
       "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 3, 12, 12)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformation = full_resolution_crop\n",
    "train_loader, test_loader = get_loaders(batch_size, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.4092\n",
      "epoch [2/100], loss:0.3377\n",
      "epoch [3/100], loss:0.3058\n",
      "epoch [4/100], loss:0.3483\n",
      "epoch [5/100], loss:0.2722\n",
      "epoch [6/100], loss:0.2946\n",
      "epoch [7/100], loss:0.2499\n",
      "epoch [8/100], loss:0.2494\n",
      "epoch [9/100], loss:0.3001\n",
      "epoch [10/100], loss:0.1993\n",
      "epoch [11/100], loss:0.2132\n",
      "epoch [12/100], loss:0.2049\n",
      "epoch [13/100], loss:0.2972\n",
      "epoch [14/100], loss:0.1959\n",
      "epoch [15/100], loss:0.2462\n",
      "epoch [16/100], loss:0.2011\n",
      "epoch [17/100], loss:0.2434\n",
      "epoch [18/100], loss:0.2658\n",
      "epoch [19/100], loss:0.2246\n",
      "epoch [20/100], loss:0.2300\n",
      "epoch [21/100], loss:0.2028\n",
      "epoch [22/100], loss:0.2029\n",
      "epoch [23/100], loss:0.2460\n",
      "epoch [24/100], loss:0.2545\n",
      "epoch [25/100], loss:0.2325\n",
      "epoch [26/100], loss:0.2252\n",
      "epoch [27/100], loss:0.2551\n",
      "epoch [28/100], loss:0.2338\n",
      "epoch [29/100], loss:0.1642\n",
      "epoch [30/100], loss:0.2235\n",
      "epoch [31/100], loss:0.1950\n",
      "epoch [32/100], loss:0.2379\n",
      "epoch [33/100], loss:0.2440\n",
      "epoch [34/100], loss:0.2408\n",
      "epoch [35/100], loss:0.2713\n",
      "epoch [36/100], loss:0.2313\n",
      "epoch [37/100], loss:0.2363\n",
      "epoch [38/100], loss:0.2542\n",
      "epoch [39/100], loss:0.2782\n",
      "epoch [40/100], loss:0.1701\n",
      "epoch [41/100], loss:0.1879\n",
      "epoch [42/100], loss:0.2589\n",
      "epoch [43/100], loss:0.2518\n",
      "epoch [44/100], loss:0.2565\n",
      "epoch [45/100], loss:0.2314\n",
      "epoch [46/100], loss:0.2601\n",
      "epoch [47/100], loss:0.2513\n",
      "epoch [48/100], loss:0.2133\n",
      "epoch [49/100], loss:0.2205\n",
      "epoch [50/100], loss:0.2386\n",
      "epoch [51/100], loss:0.2227\n",
      "epoch [52/100], loss:0.2001\n",
      "epoch [53/100], loss:0.1826\n",
      "epoch [54/100], loss:0.2644\n",
      "epoch [55/100], loss:0.2641\n",
      "epoch [56/100], loss:0.2712\n",
      "epoch [57/100], loss:0.2261\n",
      "epoch [58/100], loss:0.2290\n",
      "epoch [59/100], loss:0.2018\n",
      "epoch [60/100], loss:0.2029\n",
      "epoch [61/100], loss:0.2601\n",
      "epoch [62/100], loss:0.1618\n",
      "epoch [63/100], loss:0.2245\n",
      "epoch [64/100], loss:0.2053\n",
      "epoch [65/100], loss:0.2228\n",
      "epoch [66/100], loss:0.1712\n",
      "epoch [67/100], loss:0.2746\n",
      "epoch [68/100], loss:0.2654\n",
      "epoch [69/100], loss:0.1959\n",
      "epoch [70/100], loss:0.2104\n",
      "epoch [71/100], loss:0.2078\n",
      "epoch [72/100], loss:0.2283\n",
      "epoch [73/100], loss:0.2238\n",
      "epoch [74/100], loss:0.2457\n",
      "epoch [75/100], loss:0.2209\n",
      "epoch [76/100], loss:0.1890\n",
      "epoch [77/100], loss:0.2088\n",
      "epoch [78/100], loss:0.2030\n",
      "epoch [79/100], loss:0.2173\n",
      "epoch [80/100], loss:0.2101\n",
      "epoch [81/100], loss:0.2130\n",
      "epoch [82/100], loss:0.1821\n",
      "epoch [83/100], loss:0.2299\n",
      "epoch [84/100], loss:0.3318\n",
      "epoch [85/100], loss:0.2389\n",
      "epoch [86/100], loss:0.2200\n",
      "epoch [87/100], loss:0.2253\n",
      "epoch [88/100], loss:0.2084\n",
      "epoch [89/100], loss:0.2013\n",
      "epoch [90/100], loss:0.2318\n",
      "epoch [91/100], loss:0.2426\n",
      "epoch [92/100], loss:0.2342\n",
      "epoch [93/100], loss:0.2158\n",
      "epoch [94/100], loss:0.2128\n",
      "epoch [95/100], loss:0.2286\n",
      "epoch [96/100], loss:0.2585\n",
      "epoch [97/100], loss:0.2448\n",
      "epoch [98/100], loss:0.2380\n",
      "epoch [99/100], loss:0.1772\n",
      "epoch [100/100], loss:0.1301\n",
      "CPU times: user 4min 7s, sys: 10.5 s, total: 4min 18s\n",
      "Wall time: 14min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (img, labels) in enumerate(train_loader):\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "#         print(\"encoding batch of  images\")\n",
    "        output = model(img)\n",
    "#         print(\"computing loss\")\n",
    "        loss = criterion(output, img)\n",
    "        # ===================backward====================\n",
    "#         print(\"Backward \")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.data))\n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        in_pic = to_img(img.cpu().data)\n",
    "        save_image(pic, './cae_results/2x2-2xfc-out_image_{}.png'.format(epoch))\n",
    "        save_image(in_pic, './cae_results/2x2-2xfc-in_image_{}.png'.format(epoch))\n",
    "    if loss.data < 0.15: #arbitrary number because I saw that it works well enough\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"2x2-2xfc-layer\", \"CAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and Output for the first epoch\n",
    "\n",
    "![input](cae_results/2x2-2xfc-in_image_0.png)\n",
    "![output](cae_results/2x2-2xfc-out_image_0.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and Output for the 90th epoch\n",
    "\n",
    "![input](cae_results/2x2-2xfc-in_image_90.png)\n",
    "![output](cae_results/2x2-2xfc-out_image_90.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Results \n",
    "\n",
    "Experiments with the following configurations:\n",
    "- 2 layers with 2 convolutional stages each  <- **best result**\n",
    "- 2 layers with 2 convolutional stages each and 2 fully connected layers  <- bigger model and a bit slower to converge, but results are good too\n",
    "- 2 layers with 2 convolutional stages each with batch normalization\n",
    "- 2 layers with 4 convolutional stages each <- **worst result**\n",
    "\n",
    "\n",
    "2 layers with 4 conv stages each does not give the same results as 2 layers with 2 conv stages\n",
    "\n",
    "It not only converges MUCH faster and the models are smaller, but the actually the convergence is much better\n",
    "\n",
    "For batch normalization happens the same, without batchnorm2d converges faster and model is smaller"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
