{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Experiments - Playing With Different Architectures\n",
    "\n",
    "Sometimes we need to come back to the basis, this is the place I choose for that.\n",
    "\n",
    "Here I'll experiment with different networks on the MNIST and MNIST variants datasets trying to find relations in which I can reduce the number of parameters in comparison with a Fully Connected (FC) network.\n",
    "\n",
    "Later on, I might try with other datasets that are small enough for my GTX1080.\n",
    "\n",
    "Yes, I know, the issue is already solved for Images with Convolutional Networks but what I want to see is not that. Instead I want to understand ways in which fully connected networks can be replaced by other types of connections to minimize the number of parameters in it. This is an exploratory work to get a deeper understanding on Neural Networks (NNs) that will at least give me some fun time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ColumnNet\n",
    "\n",
    "ColumnNet experiments\n",
    "\n",
    "A neural network that contains different networks, each consisting of a column, each column can have different activation units\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network modules  to try\n",
    "from network_modules import *\n",
    "from net_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_mnist():\n",
    "    return transforms.Compose([\n",
    "#         transforms.Grayscale(),\n",
    "#         transforms.Resize((w, h)),  # this should be used ONLY if the image is bigger than this size\n",
    "        transforms.ToTensor()\n",
    "#         transforms.Normalize(0.5, 0.25)\n",
    "    ])\n",
    "# Datasets:\n",
    "# mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist())\n",
    "# mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected ColumnNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, mname, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "\n",
    "    model.to(device)\n",
    "    num_epochs = 100\n",
    "    batch_size = 128\n",
    "#     learning_rate = 0.0001\n",
    "    learning_rate = 0.001\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "#     criterion = nn.MSELoss()\n",
    "    criterion = F.nll_loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "    transformation = transform_mnist()\n",
    "    train_loader, test_loader = get_loaders(batch_size, transformation)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (img, labels) in enumerate(train_loader):\n",
    "#             print(\"shape: \", img.shape, labels.shape)\n",
    "            labels = labels.to(device)\n",
    "            img = img.to(device).view((-1,784))\n",
    "            \n",
    "#             print(\"shape2: \", img.shape)\n",
    "            # ===================forward=====================\n",
    "            #         print(\"encoding batch of  images\")\n",
    "            output = model(img)\n",
    "#             print(\"output shape: \", output.shape, labels.shape, labels[:10])\n",
    "            #         print(\"computing loss\")\n",
    "            loss = criterion(output, labels)\n",
    "            # ===================backward====================\n",
    "            #         print(\"Backward \")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # ===================log========================\n",
    "        if epoch % 20 == 0:\n",
    "            print('epoch [{}/{}], loss:{:.6f}'.format(epoch+1, num_epochs, loss.data))\n",
    "#         if epoch % 10 == 0:\n",
    "#             pic = to_img(output.cpu().data)\n",
    "#             in_pic = to_img(img.cpu().data)\n",
    "#             save_image(pic, './results/2x2-out_image_{}.png'.format(epoch))\n",
    "#             save_image(in_pic, './results/2x2-in_image_{}.png'.format(epoch))\n",
    "#         if loss.data[0] < 0.015: #arbitrary number because I saw that it works well enough\n",
    "#             print(\"loss < 0.015, breaking\")\n",
    "#             break\n",
    "#     model.save_model(mname, \"model\")\n",
    "\n",
    "    print('########################################################')\n",
    "    print('Final performance of model {} epoch [{}/{}], loss:{:.8f}'.format(mname, epoch+1, num_epochs, loss.data))\n",
    "    print('--------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnets_layers = [\n",
    "        [784,50,50],\n",
    "        [784,100,50],\n",
    "        [784,150,50],\n",
    "        [784,200,50],\n",
    "        [784,50,50,50],\n",
    "        [784,100,50,50],\n",
    "        [784,100,100,50],\n",
    "        [784,200,50,50],\n",
    "        [784,200,100,50],\n",
    "        [784,200,150,50],\n",
    "        [784,200,200,50],\n",
    "        [784,50,50,50,50],\n",
    "        [784,100,50,50,50],\n",
    "        [784,100,100,50,50],\n",
    "        [784,100,100,100,50],\n",
    "        [784,200,100,100,50],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [\n",
    "    \"relu\",\n",
    "    \"relu6\",\n",
    "    \"sigmoid\",\n",
    "    \"elu\",\n",
    "    \"leaky_relu\",\n",
    "    \"logsigmoid\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = [[fc]*len(activations) for fc in fcnets_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [ColumnNet(c, activations) for c in columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:1.098784\n",
      "epoch [21/100], loss:0.206056\n",
      "epoch [41/100], loss:0.000080\n",
      "epoch [61/100], loss:0.032990\n",
      "epoch [81/100], loss:0.001306\n",
      "########################################################\n",
      "Final performance of model [784, 50, 50] epoch [100/100], loss:0.00123214\n",
      "--------------------------------------------------------\n",
      "epoch [1/100], loss:1.367187\n",
      "epoch [21/100], loss:0.029596\n",
      "epoch [41/100], loss:0.000118\n",
      "epoch [61/100], loss:0.001094\n",
      "epoch [81/100], loss:0.001255\n",
      "########################################################\n",
      "Final performance of model [784, 100, 50] epoch [100/100], loss:0.00013174\n",
      "--------------------------------------------------------\n",
      "epoch [1/100], loss:0.403242\n",
      "epoch [21/100], loss:0.012076\n",
      "epoch [41/100], loss:0.000257\n",
      "epoch [61/100], loss:0.004873\n",
      "epoch [81/100], loss:0.000759\n",
      "########################################################\n",
      "Final performance of model [784, 150, 50] epoch [100/100], loss:0.00014679\n",
      "--------------------------------------------------------\n",
      "epoch [1/100], loss:1.438411\n",
      "epoch [21/100], loss:1.175742\n",
      "epoch [41/100], loss:0.677915\n",
      "epoch [61/100], loss:0.024253\n",
      "epoch [81/100], loss:0.000468\n",
      "########################################################\n",
      "Final performance of model [784, 200, 50] epoch [100/100], loss:0.00005428\n",
      "--------------------------------------------------------\n",
      "epoch [1/100], loss:1.736917\n",
      "epoch [21/100], loss:0.207412\n",
      "epoch [41/100], loss:0.000413\n",
      "epoch [61/100], loss:0.000363\n",
      "epoch [81/100], loss:0.005977\n",
      "########################################################\n",
      "Final performance of model [784, 50, 50, 50] epoch [100/100], loss:0.00000984\n",
      "--------------------------------------------------------\n",
      "epoch [1/100], loss:0.321369\n",
      "epoch [21/100], loss:0.281001\n",
      "epoch [41/100], loss:0.000223\n",
      "epoch [61/100], loss:0.000319\n",
      "epoch [81/100], loss:0.000864\n",
      "########################################################\n",
      "Final performance of model [784, 100, 50, 50] epoch [100/100], loss:0.00385269\n",
      "--------------------------------------------------------\n",
      "epoch [1/100], loss:1.114298\n",
      "epoch [21/100], loss:0.752717\n",
      "epoch [41/100], loss:0.264392\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(len(fcnets_layers)):\n",
    "    mname = str(fcnets_layers[i])\n",
    "    model = models[i]\n",
    "    train(model, mname)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
